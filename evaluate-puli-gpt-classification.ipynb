{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7cf339-9b56-4a2f-b8bb-3c6bd7c72266",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "peft_model_id = \"./kmonitor-gpt-fixed-loss-v2-46k\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"NYTK/PULI-GPT-3SX\", return_dict=True, load_in_8bit=True, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NYTK/PULI-GPT-3SX\")\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "model = PeftModel.from_pretrained(model, peft_model_id, device_map={'':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d682aac0-7bea-469a-a5d9-5f2ecff04c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "\n",
    "def trunc_to(descr, n):\n",
    "    if '.' in descr[int(n/2):n]:\n",
    "        return descr[:descr[:n].rfind('.')+1]\n",
    "    elif ' ' in descr[int(n/2):n]:\n",
    "        return descr[:descr[:n].rfind(' ')+1]\n",
    "    else:\n",
    "        return descr[:n]\n",
    "\n",
    "\n",
    "data = load_from_disk(\"./dataset-fixed\")\n",
    "\n",
    "# TODO better solution for labels and tokenization\n",
    "test = [tokenizer('[klasszifikáció]\\n' + trunc_to(samples[\"title\"], 150) + '\\n\\n' + trunc_to(samples[\"description\"], 700) + '\\n\\n' + trunc_to(samples['text'], 2500) + '\\n\\n###\\n\\ntéma:', return_tensors=\"pt\") for samples in data['test']]\n",
    "labels = []\n",
    "for i, l in enumerate(test):\n",
    "    if l:\n",
    "        labels.append(data['test'][i]['theme'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3fd65a-caad-407d-8c40-9f3ad18be004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "invalid = 0\n",
    "\n",
    "genconf = GenerationConfig(\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "for n, d in tqdm(enumerate(test), total=3618):\n",
    "    with torch.cuda.amp.autocast():\n",
    "        d = d.to('cuda')\n",
    "        output_tokens = model.generate(**d, max_new_tokens=2, pad_token_id=tokenizer.eos_token_id, generation_config=genconf)\n",
    "        d = d.to('cpu')\n",
    "        result = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "        r = result[result.rfind('téma:')+len('téma:'):]\n",
    "        if len(r.strip()) > 0:\n",
    "            r = r.strip()[0]\n",
    "\n",
    "        if r != 'k' and r != 'e':\n",
    "            invalid += 1\n",
    "        if r == 'k':\n",
    "            if labels[n] == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "        else:\n",
    "            if labels[n] == 0:\n",
    "                tn += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        if n % 100 == 0:\n",
    "            print(tp, fp, tn, fn)\n",
    "            print((tp+tn)/(tp+fp+tn+fn))\n",
    "print(tp, fp, tn, fn)\n",
    "print((tp+tn)/(tp+fp+tn+fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0de6540-d011-4c3a-90af-e25b5d3b1ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "genconf = GenerationConfig(\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "total = 0\n",
    "for i in range(10):\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    invalid = 0\n",
    "\n",
    "    for n, d in tqdm(enumerate(test), total=3618):\n",
    "        with torch.cuda.amp.autocast():\n",
    "            d = d.to('cuda')\n",
    "            output_tokens = model.generate(**d, max_new_tokens=2, pad_token_id=tokenizer.eos_token_id, generation_config=genconf)\n",
    "            d = d.to('cpu')\n",
    "            result = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "            r = result[result.rfind('téma:')+len('téma:'):]\n",
    "            if len(r.strip()) > 0:\n",
    "                r = r.strip()[0]\n",
    "\n",
    "            if r != 'k' and r != 'e':\n",
    "                invalid += 1\n",
    "            if r == 'k':\n",
    "                if labels[n] == 1:\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    fp += 1\n",
    "            else:\n",
    "                if labels[n] == 0:\n",
    "                    tn += 1\n",
    "                else:\n",
    "                    fn += 1\n",
    "    print(tp, fp, tn, fn)\n",
    "    print((tp+tn)/(tp+fp+tn+fn))\n",
    "    total += (tp+tn)/(tp+fp+tn+fn)\n",
    "print(f'Test Accuracy: {total / 10: .5f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipykernel",
   "language": "python",
   "name": "ipykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
